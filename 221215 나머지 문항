1. 적대적 예제 실행
오류가 난 부분은 '미리 학습된 모델을 읽어오기' 부분인데
https://github.com/pytorch/vision/issues/3497 참고해 확인한 결과
colab googlecolab/colabtools#1889 에서 torchvision으로 더 이상 MNIST 데이터세트를 다운로드할 수 없다고 나와있습니다.

2. 결과 분석 및 정리
첫 번째 결과는 정확도 vs 엡실론을 도식화 한 것 이다. 엡실론이 증가함에 따라 테스트 정확도가 감소할 것으로 예상한다.
이는 학습을 더 진행해 갈수록 엡실론이 클수록 손실을 극대화 할 방향으로 진행되기 때문이다다. 엡실론 값이 선형적으로 분포하더라도 곡선의 추세는 선형의 형태가 아니다.

샘플 적대적 예제
엡실론이 증가할수록 테스트 정확도는 떨어진다. 그러나 작은 변화는 더 쉽게 인식할 수 있게 된다.
정확도 저하와 공격자가 고려해야 하는 이해도 사이에는 상충 관계가 있다.
여기서 각 엡실론 값에서 성공적인 대적 사례를 보이는 몇 가지 예를 보면
아래 이미지의 첫번째로 열은 ϵ = 0인 예제들로 작은 변화가 없는 원본의 깨끗한 이미지들을 나타낸다.
각 이미지의 위의 글자는 원래 분류 결과 -> 적대적 분류 결과를 나타낸다.
ϵ = 0.15 에서 작은 변화가 눈에 띄기 시작하고 ϵ = 0.3 에서는 확실해 보인다.
그러나 모든 경우에 대해서 노이즈가 추가되었더라도 사람은 올바르게 분류를 수행할 수 있다.

적대적 공격은 딥러닝의 심층신경망을 이용한 모델에 적대적 교란(Adversarial Pertubation)을 
적용하여 오분류를 발생시키는 것을 의미한다.
또 인공지능에 입력되는 데이터를 사용자가 눈치 채지 못할 만큼 수정하여 모델에 오작동을 일으키는 공격이다.

정상적인 입력에 사람의 눈으로 식별되지 않을 만큼 작은 노이즈를 추가해서 딥러닝 모델을 속이는 공격이다. 
이러한 공격은 치명적인 결과를 초래할 수 있다. 


3. 발전 및 활용 방법
AI가 잘못된 의사결정을 하도록 하는 것이 적대적 공격의 목적이다.
교통 표지판에 작은 노이즈를 추가해 자율주행에서 사용되는 딥러닝 모델을 공격하면 교통사고를 유발할 수 있다.

신뢰도 감소(Confidence reduction)는 예측 신뢰도를 낮추는 공격이다.
모델에 대한 예측 신뢰도를 감소시키는 것이다. 
예를 들어 자율자동차가 Stop 표지판을 높은 확률로 분류하는 것을 낮은 확률로 분류하도록 하는 것이다.

오분류(Misclassification)는 오답을 유발하는 공격이다.
오분류는 A라는 집단을 n₁ , n₂, n₃ , ... 등의 불특정 집단으로 
오분류하도록 하는 것이다. 
예를 들어 자율 자동차에서 Stop 표지판을 다른 것으로 분류하도록 하는 것이다.

출력 오분류(Targeted Misclassification)는 의도한 오답을 유발하는 공격이다
출력 오분류는 N(n₁ , n₂, n₃ , ...)이라는 모든 집단을 A라는 하나의 집단으로만 오분류하도록 하는 것이다. 
예를 들어 자율자동차가 인식하는 모든 표지판을 Go로 분류하도록 하는 것이다.

입력 및 출력 오분류(Source/Target Misclassification)는 입력에 따라서 오답을 유발하는 공격이다
입력 및 출력 오분류는 집단 A는 n₁ 으로, 집단 B는 n₂ 등으로 1:1 매칭하여 오분류하도록 하는 것이다. 
예를 들어 자율자동차가 Stop라는 표지판을 Go로 분류하도록 하는 것이다.
